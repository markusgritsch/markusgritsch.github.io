<html><head><meta http-equiv="Content-Type" content="text/html;charset=utf-8" /><link href="style_edited.css" rel="stylesheet" type="text/css" /><title>Understanding Digital Signal Processing</title>
<style type="text/css">
body {
  -webkit-text-size-adjust: 260%; /* text */
  zoom: 1.7; /* graphics */
  font-family: "BookerlyLCD";
}
a {
  color: #005090;
  text-decoration: none;
}
a {font-variant-numeric: oldstyle-nums proportional-nums;}
p.caption {font-variant-numeric: oldstyle-nums proportional-nums;}
</style>
</head><body>


<div class="calibre" id="calibre_link-572">
<p class="calibre3"><a id="calibre_link-2482"></a></p>
<h2 class="calibre8"><a id="calibre_link-2483" class="calibre5"></a>Contents</h2>
<p class="toc-appendix"><strong class="calibre9"><a href="#calibre_link-537">D MEAN, VARIANCE, AND STANDARD DEVIATION</a></strong></p>
<p class="toc-section"><a href="#calibre_link-565">D.1 Statistical Measures</a></p>
<p class="toc-section"><a href="#calibre_link-31">D.2 Statistics of Short Sequences</a></p>
<p class="toc-section"><a href="#calibre_link-94">D.3 Statistics of Summed Sequences</a></p>
<p class="toc-section"><a href="#calibre_link-726">D.4 Standard Deviation (RMS) of a Continuous Sinewave</a></p>
<p class="toc-section"><a href="#calibre_link-727">D.5 Estimating Signal-to-Noise Ratios</a></p>
<p class="toc-section"><a href="#calibre_link-556">D.6 The Mean and Variance of Random Functions</a></p>
<p class="toc-section"><a href="#calibre_link-728">D.7 The Normal Probability Density Function</a></p>

</div>


<div class="calibre" id="calibre_link-537">
<p class="calibre3"><a id="calibre_link-27"></a></p>
<h2 class="calibre8"><a id="calibre_link-2449" class="calibre5"></a>D Mean, Variance, and Standard Deviation</h2>
<p class="calibre3">In our studies, we’re often forced to consider noise functions. These are descriptions of noise signals that we cannot explicitly describe with a time-domain equation. Noise functions can be quantified, however, in a worthwhile way using the statistical measures of mean, variance, and standard deviation. Although here we only touch on the very broad and important field of statistics, we will describe why, how, and when to use these statistical indicators, so that we can add them to our collection of signal analysis tools. First we’ll determine how to calculate these statistical values for a series of discrete data samples, cover an example using a continuous analytical function, and conclude this appendix with a discussion of the probability density functions of several random variables that are common in the field of digital signal processing. So let’s proceed by sticking our toes in the chilly waters of the mathematics of statistics to obtain a few definitions.</p>
<p class="calibre3"><a id="calibre_link-565"></a></p>
<h3 class="calibre6">D.1 Statistical Measures</h3>
<p class="calibre3">Consider a continuous sinusoid having a frequency of <em class="calibre7">f</em><sub class="calibre12">o</sub> Hz with a peak amplitude of <em class="calibre7">A<sub class="calibre12">p</sub></em> expressed by the equation</p>
<p class="caption"><a id="calibre_link-538"></a>(D-1)</p>
<p class="image"><img src="images/000411.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3"><a href="#calibre_link-538">Equation (D-1)</a> completely specifies <em class="calibre7">x</em>(<em class="calibre7">t</em>)&mdash;that is, we can determine <em class="calibre7">x</em>(<em class="calibre7">t</em>)’s exact value at any given instant in time. For example, when time <em class="calibre7">t</em> = 1/4<em class="calibre7">f</em><sub class="calibre12">o</sub>, we <a id="calibre_link-1989"></a>know that <em class="calibre7">x</em>(<em class="calibre7">t</em>)’s amplitude will be <em class="calibre7">A<sub class="calibre12">p</sub></em>, and at the later time <em class="calibre7">t</em> = 1/2<em class="calibre7">f</em><sub class="calibre12">o</sub>, <em class="calibre7">x</em>(<em class="calibre7">t</em>)’s amplitude will be zero. On the other hand, we have no definite way to express the successive values of a random function or of random noise.<sup class="calibre10"><a id="calibre_link-540"></a><a href="#calibre_link-539">†</a></sup> There’s no equation like <a href="#calibre_link-538">Eq. (D-1)</a> available to predict future noise-amplitude values, for example. (That’s why they call it random noise.) Statisticians have, however, developed powerful mathematical tools to characterize several properties of random functions. The most important of these properties have been given the names <em class="calibre7">mean</em>, <em class="calibre7">variance</em>, and <em class="calibre7">standard deviation</em>.</p>
<p class="footnotes"><a id="calibre_link-539"></a><sup class="calibre11"><a href="#calibre_link-540">†</a></sup> We define <em class="calibre7">random noise</em> to be unwanted, unpredictable disturbances contaminating a signal or a data sequence of interest.</p>
<p class="calibre3">Mathematically, the <em class="calibre7">sample mean</em>, or <em class="calibre7">average</em>, of <em class="calibre7">N</em> separate values of a sequence <em class="calibre7">x</em>, denoted <em class="calibre7">x</em><sub class="calibre12">ave</sub>, is defined as<a href="#calibre_link-541">[1]</a></p>
<p class="caption"><a id="calibre_link-542"></a>(D-2)</p>
<p class="image"><img src="images/001262.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3"><a href="#calibre_link-542">Equation (D-2)</a>, already familiar to most people, merely states that the average of a sequence of <em class="calibre7">N</em> numbers is the sum of those numbers divided by <em class="calibre7">N</em>. Graphically, the average can be depicted as that value around which a series of sample values cluster, or congregate, as shown in <a href="#calibre_link-543">Figure D-1</a>. If the eight values depicted by the dots in <a href="#calibre_link-543">Figure D-1</a> represent some measured quantity and we applied those values to <a href="#calibre_link-542">Eq. (D-2)</a>, the average of the series is 5.17, as shown by the dotted line.</p>
<p class="caption"><a id="calibre_link-543"></a><strong class="calibre9">Figure D-1</strong> Average of a sequence of eight values.</p>
<p class="image"><img src="images/001423.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">An interesting property of the average (mean value) of an <em class="calibre7">x</em>(<em class="calibre7">n</em>) sequence is that <em class="calibre7">x</em><sub class="calibre12">ave</sub> is the value that makes the sum of the differences between <em class="calibre7">x</em>(<em class="calibre7">n</em>) and <em class="calibre7">x</em><sub class="calibre12">ave</sub> equal to zero. That is, the sum of the sequence <em class="calibre7">d</em><sub class="calibre12">iff</sub>(<em class="calibre7">n</em>) = <em class="calibre7">x</em>(<em class="calibre7">n</em>) &ndash; <em class="calibre7">x</em><sub class="calibre12">ave</sub> is zero.</p>
<p class="calibre3"><a id="calibre_link-2166"></a>Now that we’ve defined <em class="calibre7">average</em>, another key definition is the variance of a sequence, σ<sup class="calibre10">2</sup>, defined as</p>
<p class="caption"><a id="calibre_link-544"></a>(D-3)</p>
<p class="image"><img src="images/000301.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">Sometimes in the literature we’ll see σ<sup class="calibre10">2</sup> defined with a 1/(<em class="calibre7">N</em>&ndash;1) factor before the summation instead of the 1/<em class="calibre7">N</em> factor in <a href="#calibre_link-544">Eq. (D-3)</a>. In a moment we’ll explain why this is so.</p>
<p class="calibre3">Variance is a very important concept because it’s the yardstick with which we measure, for example, the effect of quantization errors and the usefulness of signal-averaging algorithms. It gives us an idea how the aggregate values in a sequence fluctuate around the sequence’s average and provides us with a well-defined quantitative measure of those fluctuations. Mathematicians call those fluctuations the <em class="calibre7">dispersion</em> of the sequence. (Because the positive square root of the variance, the standard deviation, is typically denoted as σ in the literature, we’ll use the conventional notation of σ<sup class="calibre10">2</sup> for the variance.)</p>
<p class="calibre3"><a href="#calibre_link-544">Equation (D-3)</a> looks a bit perplexing if you haven’t seen it before. Its meaning becomes clear if we examine it carefully. The <em class="calibre7">x</em>(1) &ndash; <em class="calibre7">x</em><sub class="calibre12">ave</sub> value in the bracket, for example, is the difference between the <em class="calibre7">x</em>(1) value and the sequence average <em class="calibre7">x</em><sub class="calibre12">ave</sub>. For any sequence value <em class="calibre7">x</em>(<em class="calibre7">n</em>), the <em class="calibre7">x</em>(<em class="calibre7">n</em>) &ndash; <em class="calibre7">x</em><sub class="calibre12">ave</sub> difference, which we denote as Δ(<em class="calibre7">n</em>), can be either positive or negative, as shown in <a href="#calibre_link-545">Figure D-2</a>. Specifically, the differences Δ(1), Δ(2), Δ(3), and Δ(8) are negative because their corresponding sequence values are below the sequence average shown by the dotted line. If we replace the <em class="calibre7">x</em>(<em class="calibre7">n</em>) &ndash; <em class="calibre7">x</em><sub class="calibre12">ave</sub> difference terms in <a href="#calibre_link-544">Eq. (D-3)</a> with Δ(<em class="calibre7">n</em>) terms, the variance can be expressed as</p>
<p class="caption"><a id="calibre_link-546"></a>(D-4)</p>
<p class="image"><img src="images/001139.jpg" alt="image" class="calibre2" /></p>
<p class="caption"><a id="calibre_link-545"></a><strong class="calibre9">Figure D-2</strong> Difference values Δ(<em class="calibre7">n</em>) of the sequence in <a href="#calibre_link-543">Figure D-1</a>.</p>
<p class="image"><img src="images/000460.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">The reader might wonder why the squares of the differences are summed, instead of just the differences themselves. This is because, by the very nature of the definition of <em class="calibre7">x</em><sub class="calibre12">ave</sub>, the sum of the Δ(<em class="calibre7">n</em>) difference samples will always be zero. Because we need an unsigned measure of each difference, we use the difference-squared terms as indicated by <a href="#calibre_link-546">Eq. (D-4)</a>. In that way, individual Δ(<em class="calibre7">n</em>) difference terms will contribute to the overall variance regardless of whether the difference is positive or negative. Plugging the Δ(<em class="calibre7">n</em>) values from the example sequence in <a href="#calibre_link-545">Figure D-2</a> into <a href="#calibre_link-546">Eq. (D-4)</a>, we get a <a id="calibre_link-2008"></a>variance value of 0.34. Another useful measure of a signal sequence is the square root of the variance known as the <em class="calibre7">standard deviation</em>. Taking the square root of <a href="#calibre_link-544">Eq. (D-3)</a> to get the standard deviation σ,</p>
<p class="caption"><a id="calibre_link-3494"></a>(D-5)</p>
<p class="image"><img src="images/000154.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">So far, we have three measurements to use in evaluating a sequence of values: the average <em class="calibre7">x</em><sub class="calibre12">ave</sub>, the variance σ<sup class="calibre10">2</sup>, and the standard deviation σ. Where <em class="calibre7">x</em><sub class="calibre12">ave</sub> indicates around what constant level the individual sequence values vary, σ<sup class="calibre10">2</sup> is a measure of the magnitude of the noise fluctuations around the average <em class="calibre7">x</em><sub class="calibre12">ave</sub>. If the sequence represents a series of random signal samples, we can say that <em class="calibre7">x</em><sub class="calibre12">ave</sub> specifies the average, or constant, value of the signal. The variance σ<sup class="calibre10">2</sup> is the magnitude squared, or power, of the fluctuating component of the signal. The standard deviation, then, is an indication of the magnitude of the fluctuating component of the signal.</p>
<p class="calibre3"><a id="calibre_link-31"></a></p>
<h3 class="calibre6">D.2 Statistics of Short Sequences</h3>
<p class="calibre3">In this section we discuss a subtle issue regarding the variance of a discrete sequence. The variance <a href="#calibre_link-544">Eq. (D-3)</a> is only <em class="calibre7">exactly</em> correct if <em class="calibre7">N</em> is infinitely large. When <em class="calibre7">N</em> is a small number and we’re computing an [<em class="calibre7">x</em>(4)&ndash;<em class="calibre7">x</em><sub class="calibre12">ave</sub>] term, for example, that [<em class="calibre7">x</em>(4)&ndash;<em class="calibre7">x</em><sub class="calibre12">ave</sub>] value is too highly influenced (biased) by the single <em class="calibre7">x</em>(4) sample. This results in an [<em class="calibre7">x</em>(4)&ndash;<em class="calibre7">x</em><sub class="calibre12">ave</sub>] value that’s slightly smaller than it should be<a href="#calibre_link-547">[2]</a>. As such, <a href="#calibre_link-544">Eq. (D-3)</a> is often called a <em class="calibre7">biased estimate</em> of the true variance of <em class="calibre7">x</em>(<em class="calibre7">n</em>). Mathematicians have determined that using a 1/(<em class="calibre7">N</em>&ndash;1) factor, called <em class="calibre7">Bessel’s correction,</em> before the summation in <a href="#calibre_link-544">Eq. (D-3)</a> yields a more accurate <a id="calibre_link-2451"></a>estimation of the true variance of the infinite-length sequence <em class="calibre7">x</em>(<em class="calibre7">n</em>), when we use only <em class="calibre7">N</em> samples of <em class="calibre7">x</em>(<em class="calibre7">n</em>) to estimate the true variance. That is,</p>
<p class="caption"><a id="calibre_link-548"></a>(D-6)</p>
<p class="image"><img src="images/001029.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3"><a href="#calibre_link-548">Equation (D-6)</a> is called an <em class="calibre7">unbiased estimate</em> of the variance of <em class="calibre7">x</em>(<em class="calibre7">n</em>). However, when <em class="calibre7">N</em> is greater than, say, 100, as it often is in real-world applications, the difference between <a href="#calibre_link-544">Eqs. (D-3)</a> and <a href="#calibre_link-548">(D-6)</a> will have little practical significance.</p>
<p class="calibre3">We can justify that claim by showing an example of the percent difference in using <a href="#calibre_link-544">Eqs. (D-3)</a> and <a href="#calibre_link-548">(D-6)</a>, as a function of the <em class="calibre7">x</em>(<em class="calibre7">n</em>) sequence length <em class="calibre7">N</em>, as the solid curve in <a href="#calibre_link-549">Figure D-3</a>. Considering the unbiased variance to be correct (zero error), the solid error curve in <a href="#calibre_link-549">Figure D-3</a> shows how much smaller (negative percent error) the biased variance will be compared to the unbiased variance when <em class="calibre7">x</em>(<em class="calibre7">n</em>) is Gaussian (to be described later) distributed random noise of unity variance. For instance, the percent error between the biased and the unbiased variance estimates is roughly &ndash;1 percent when <em class="calibre7">N</em> = 100. The dashed curve in <a href="#calibre_link-549">Figure D-3</a> is equal to &ndash;100 percent times the true <em class="calibre7">x</em>(<em class="calibre7">n</em>) variance divided by <em class="calibre7">N</em>, so we can say that the percent error in using <a href="#calibre_link-544">Eq. (D-3)</a> compared to <a href="#calibre_link-548">Eq. (D-6)</a> is roughly</p>
<p class="caption"><a id="calibre_link-3495"></a>(D-7)</p>
<p class="image"><img src="images/000081.jpg" alt="image" class="calibre2" /></p>
<p class="caption"><a id="calibre_link-549"></a><strong class="calibre9">Figure D-3</strong> Percent error in <a href="#calibre_link-544">Eq. (D-3)</a> relative to <a href="#calibre_link-548">Eq. (D-6)</a>.</p>
<p class="image"><img src="images/000045.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">The bottom line here is that <a href="#calibre_link-548">Eq. (D-6)</a> should be considered for use in computing the variances of discrete sequences when <em class="calibre7">N</em> is small. <a href="#calibre_link-550">Section 13.35</a> discusses a computationally efficient, and memory-saving, way to compute variances.</p>
<p class="calibre3"><a id="calibre_link-94"></a></p>
<h3 class="calibre6"><a id="calibre_link-2450" class="calibre5"></a>D.3 Statistics of Summed Sequences</h3>
<p class="calibre3">Here we discuss the statistical effects of adding two sequences. This material has great utility in noise-reduction operations. If we add two equal-length independent (uncorrelated) sequences <em class="calibre7">q</em>(<em class="calibre7">n</em>) and <em class="calibre7">r</em>(<em class="calibre7">n</em>), such that</p>
<p class="caption"><a id="calibre_link-3496"></a>(D-8)</p>
<p class="image"><img src="images/000925.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">thanks to the good work of dead mathematicians we can say<a href="#calibre_link-551">[3]</a>:</p>
<p class="indenthangingb">• The average (mean) of the <em class="calibre7">p</em>(<em class="calibre7">n</em>) sequence is equal to the sum of the individual averages of the <em class="calibre7">q</em>(<em class="calibre7">n</em>) and <em class="calibre7">r</em>(<em class="calibre7">n</em>) sequences.</p>
<p class="indenthangingb">• The variance of the <em class="calibre7">p</em>(<em class="calibre7">n</em>) sequence is equal to the sum of the individual variances of the <em class="calibre7">q</em>(<em class="calibre7">n</em>) and <em class="calibre7">r</em>(<em class="calibre7">n</em>) sequences. That is,</p>
<p class="image"><img src="images/000040.jpg" alt="image" class="calibre2" /></p>
<p class="indentpara6">This means that if we consider the variances of two signals as being measures of their noise powers, then when two noisy signals are added, the resultant signal’s noise power is the sum of the two individual noise powers.</p>
<p class="indenthangingb">• The variance of <em class="calibre7">C</em> · <em class="calibre7">p</em>(<em class="calibre7">n</em>) = <em class="calibre7">C</em> · <em class="calibre7">q</em>(<em class="calibre7">n</em>) + <em class="calibre7">C</em> · <em class="calibre7">r</em>(<em class="calibre7">n</em>), where <em class="calibre7">C</em> is a constant, is <em class="calibre7">C</em><sup class="calibre10">2</sup> times the variance of the <em class="calibre7">p</em>(<em class="calibre7">n</em>) sequence, or</p>
<p class="image"><img src="images/001781.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">The above properties are related to a key characteristic of sampled signals that we can use for noise reduction by way of averaging. Assume we have an infinitely long <em class="calibre7">x</em>(<em class="calibre7">n</em>) sequence contaminated with uncorrelated noise, and the variance of <em class="calibre7">x</em>(<em class="calibre7">n</em>) is <em class="calibre7">K</em>. If we extract <em class="calibre7">N</em> blocks of samples from <em class="calibre7">x</em>(<em class="calibre7">n</em>), with each block sequence being <em class="calibre7">M</em> samples in length, and average those <em class="calibre7">N</em> sequences, the variance of the resultant single <em class="calibre7">M</em>-sample average sequence is</p>
<p class="caption"><a id="calibre_link-552"></a>(D-9)</p>
<p class="image"><img src="images/001787.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">The square root of <a href="#calibre_link-552">Eq. (D-9)</a> yields the standard deviation of the single <em class="calibre7">M</em>-sample average sequence as</p>
<p class="caption"><a id="calibre_link-3497"></a><a id="calibre_link-553"></a>(D-10)</p>
<p class="image"><img src="images/001763.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">where σ<em class="calibre7"><sub class="calibre12">x</sub></em> is the standard deviation of the original <em class="calibre7">x</em>(<em class="calibre7">n</em>) sequence.</p>
<p class="calibre3">As an example of <a href="#calibre_link-553">Eq. (D-10)</a>, say that we have an <em class="calibre7">x</em>(<em class="calibre7">n</em>) sequence and compute the average of the first <em class="calibre7">N</em> samples of <em class="calibre7">x</em>(<em class="calibre7">n</em>), <em class="calibre7">x</em>(0) through <em class="calibre7">x</em>(<em class="calibre7">N</em>&ndash;1), to produce an <em class="calibre7">x</em><sub class="calibre12">ave</sub>(0) sample. Next we compute the average of the second set of <em class="calibre7">N</em> samples of <em class="calibre7">x</em>(<em class="calibre7">n</em>), <em class="calibre7">x</em>(<em class="calibre7">N</em>) through <em class="calibre7">x</em>(2<em class="calibre7">N</em>&ndash;1), to produce an <em class="calibre7">x</em><sub class="calibre12">ave</sub>(1) sample, and so on. If the standard deviation of an <em class="calibre7">x</em>(<em class="calibre7">n</em>) sequence, having an average value of 10 and standard deviation σ<em class="calibre7"><sub class="calibre12">x</sub></em> = 4, <a href="#calibre_link-554">Figure D-4</a> shows the <em class="calibre7">N</em> = 4-point averaged <em class="calibre7">x</em><sub class="calibre12">ave</sub>(<em class="calibre7">n</em>) sequence having an average value of 10 and a reduced standard deviation of σ<em class="calibre7"><sub class="calibre12">x</sub></em>/<em class="calibre7">N</em> = 4/2 = 2. <a href="#calibre_link-225">Chapter 11</a> gives practical examples of using <a href="#calibre_link-553">Eq. (D-10)</a> in real-world situations.</p>
<p class="caption"><a id="calibre_link-554"></a><strong class="calibre9">Figure D-4</strong> <em class="calibre7">x</em><sub class="calibre12">ave</sub>(<em class="calibre7">n</em>) sequence when <em class="calibre7">N</em> = 4.</p>
<p class="image"><img src="images/000895.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">On a practical note, if <em class="calibre7">x</em><sub class="calibre12">s</sub>(<em class="calibre7">n</em>) are signal samples and <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>) are noise samples, we can think of the <em class="calibre7">x</em>(<em class="calibre7">n</em>) samples in <a href="#calibre_link-552">Eqs. (D-9)</a> and <a href="#calibre_link-553">(D-10)</a> as being represented by <em class="calibre7">x</em>(<em class="calibre7">n</em>) = <em class="calibre7">x</em><sub class="calibre12">s</sub>(<em class="calibre7">n</em>) + <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>). The notion of contaminating noise being uncorrelated means that all the <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>) noise samples are independent from each other, which implies that no information about any one noise sample can be determined from knowledge of any of the other noise samples. This assumption is not always valid if a noisy <em class="calibre7">x</em>(<em class="calibre7">n</em>) signal has been filtered. With lowpass filtering, adjacent noise samples will be correlated (their amplitudes will be similar); the narrower the lowpass filter’s passband, the more adjacent noise samples tend to be correlated. If the lowpass filter’s passband is wide relative to half the sample rate (<em class="calibre7">f<sub class="calibre12">s</sub></em>/2), then the correlation among noise samples will be low and the noise samples can be considered uncorrelated. If the lowpass filter’s passband is very narrow relative to <em class="calibre7">f<sub class="calibre12">s</sub></em>/2, then averaging is not as effective as we might expect from <a href="#calibre_link-552">Eqs. (D-9)</a> and <a href="#calibre_link-553">(D-10)</a>.</p>
<p class="calibre3">We have discussed many statistical measures of real-valued discrete sequences, so <a href="#calibre_link-555">Table D-1</a> compiles what we’ve learned so far. The <em class="calibre7">x</em>(<em class="calibre7">n</em>) sequence <a id="calibre_link-1990"></a>in the table can be an information-carrying signal, a noise-only signal, or a combination of the two.</p>
<p class="caption"><a id="calibre_link-555"></a><strong class="calibre9">Table D-1</strong> Statistical Measures of Real-Valued Sequences</p>
<p class="image"><img src="images/001120.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3"><a id="calibre_link-726"></a></p>
<h3 class="calibre6">D.4 Standard Deviation (RMS) of a Continuous Sinewave</h3>
<p class="calibre3">In computing the average power in electric circuits, for sinewave signals engineers often use a parameter called the <em class="calibre7">rms</em> value of the sinewave. That parameter, <em class="calibre7">x</em><sub class="calibre12">rms</sub>, for discrete samples is defined as</p>
<p class="caption"><a id="calibre_link-49"></a>(D-11)</p>
<p class="image"><img src="images/000317.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">The <em class="calibre7">x</em>(<em class="calibre7">n</em>)<sub class="calibre12">rms</sub> in <a href="#calibre_link-49">Eq. (D-11)</a> is the square <span class="underline">r</span>oot of the <span class="underline">m</span>ean (average) of the <span class="underline">s</span>quares of the sequence <em class="calibre7">x</em>(<em class="calibre7">n</em>). For a continuous sinusoid <em class="calibre7">x</em>(<em class="calibre7">t</em>) = <em class="calibre7">A</em><sub class="calibre12">p</sub>sin(2π<em class="calibre7">ft</em>) = <em class="calibre7">A</em><sub class="calibre12">p</sub>sin(ωt) whose average value is zero, <em class="calibre7">x</em><sub class="calibre12">rms</sub> is <em class="calibre7">x</em><sub class="calibre12">rms-sine</sub> defined as</p>
<p class="caption"><a id="calibre_link-2454"></a><a id="calibre_link-50"></a>(D-12)</p>
<p class="image"><img src="images/000693.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">This <em class="calibre7">x</em><sub class="calibre12">rms-sine</sub> expression is a lot easier to use for calculating average power dissipation in circuit elements than performing the integral of more complicated expressions. When a signal’s average value is zero, then its rms value is equal to the signal’s standard deviation. The variance of a sinewave is, of course, the square of <a href="#calibre_link-50">Eq. (D-12)</a>.</p>
<p class="calibre3">We’ve provided the equations for the mean (average) and variance of a sequence of discrete values, introduced an expression for the standard deviation or rms of a sequence, and given an expression for the rms value of a continuous sinewave. The next question is “How can we characterize random functions for which there are no equations to predict their values and we have no discrete sample values with which to work?” The answer is that we must use probability density functions. Before we do that, in <a href="#calibre_link-556">Section D.6</a>, let’s first show how to use our statistical measures to estimate the signal-to-noise ratio of a discrete signal.</p>
<p class="calibre3"><a id="calibre_link-727"></a></p>
<h3 class="calibre6">D.5 Estimating Signal-to-Noise Ratios</h3>
<p class="calibre3">Given the above statistics of sampled signals, we now discuss a widely used way to quantify the quality of a noise-contaminated signal. By “quality” we mean the difference between listening to a recording of the Beatles’ song “Hey Jude” on your iPod in a library and listening to the song while standing next to a running jet engine. We quantify the quality of a noise-contaminated signal by measuring, or estimating, its signal-power-to-noise-power ratio (SNR). The SNR of a signal is the ratio of the power of the noise-free signal over the power of the noise, or</p>
<p class="caption"><a id="calibre_link-3498"></a>(D-13)</p>
<p class="image"><img src="images/001554.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3"><a id="calibre_link-2452"></a>To illustrate the notion of SNR, the following list shows the SNRs (in dB) of a few common signal processing devices:</p>
<p class="image"><img src="images/000916.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">The SNR of a signal can be estimated in either the time domain or the frequency domain. We discuss those operations next.</p>
<p class="calibre3"><a id="calibre_link-3499"></a></p>
<h4 class="calibre13">D.5.1 Estimating SNR in the Time Domain</h4>
<p class="calibre3">We can estimate, by way of time-domain measurement, the SNR of a signal based on time-domain sample values. If <em class="calibre7">x</em><sub class="calibre12">s</sub>(<em class="calibre7">n</em>) are real-valued signal samples and <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>) are real-valued noise samples, the SNR of a signal <em class="calibre7">x</em>(<em class="calibre7">n</em>) = <em class="calibre7">x</em><sub class="calibre12">s</sub>(<em class="calibre7">n</em>) + <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>) is</p>
<p class="caption"><a id="calibre_link-558"></a>(D-14)</p>
<p class="image"><img src="images/000578.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">where the divide-by-<em class="calibre7">N</em> operations are shown for clarity but need not be performed because they cancel in the numerator and denominator. If we know the variances of <em class="calibre7">x</em><sub class="calibre12">s</sub>(<em class="calibre7">n</em>) and <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>), we can express the SNR of the fluctuating (AC) portion of a signal as</p>
<p class="caption"><a id="calibre_link-2453"></a><a id="calibre_link-559"></a>(D-15)</p>
<p class="image"><img src="images/001438.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">In practice signal powers can vary over many orders of magnitude. For example, military radar systems transmit signals whose power is measured in megawatts, while the signal received by your cell phone antenna is measured in microwatts. That’s 12 orders of magnitude! As such, it’s both convenient and common to describe signal power and noise power logarithmically using decibels. (Decibels are discussed in <a href="#calibre_link-296">Appendix E</a>.) We express signal-to-noise ratios measured in decibels (dB) as</p>
<p class="caption"><a id="calibre_link-557"></a>(D-16)</p>
<p class="image"><img src="images/000474.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">where the SNR term in <a href="#calibre_link-557">Eq. (D-16)</a> is the SNR value from <a href="#calibre_link-558">Eqs. (D-14)</a> or <a href="#calibre_link-559">(D-15)</a>. If we know the rms values of <em class="calibre7">x</em><sub class="calibre12">s</sub>(<em class="calibre7">n</em>) and <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>), then we can express a signal’s SNR in dB as</p>
<p class="caption"><a id="calibre_link-560"></a>(D-17)</p>
<p class="image"><img src="images/001335.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">Because the ratio in <a href="#calibre_link-560">Eq. (D-17)</a> is in terms of amplitudes (voltages or currents), rather than powers, we’re forced to use the factor of 20 in computing <em class="calibre7">SNR</em><sub class="calibre12">dB</sub> based on rms values. If we know the standard deviations of <em class="calibre7">x</em><sub class="calibre12">s</sub>(<em class="calibre7">n</em>) and <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>), we can express the SNR of the fluctuating (AC) portion of a signal in dB as</p>
<p class="caption"><a id="calibre_link-3500"></a>(D-18)</p>
<p class="image"><img src="images/000371.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">The values for linear SNR, <a href="#calibre_link-558">Eq. (D-14)</a>, are always positive, but values for <em class="calibre7">SNR</em><sub class="calibre12">dB</sub> can be positive or negative. For example, if a signal’s linear SNR is 4, then its <em class="calibre7">SNR</em><sub class="calibre12">dB</sub> is 10 · log<sub class="calibre12">10</sub>(4) = 6 dB. If a signal’s linear SNR is 1/4, then its <em class="calibre7">SNR</em><sub class="calibre12">dB</sub> is 10 · log<sub class="calibre12">10</sub>(1/4) = &ndash;6 dB.</p>
<p class="calibre3"><a id="calibre_link-3501"></a></p>
<h4 class="calibre13">D.5.2 Estimating SNR in the Frequency Domain</h4>
<p class="calibre3">We can obtain a rough estimate of the SNR of a signal based on its frequency-domain characteristics. The standard procedure for doing so is as follows: Assume we have <em class="calibre7">N</em> = 100 samples of the noisy 986 Hz real-valued <em class="calibre7">x</em>(<em class="calibre7">n</em>) sinusoid, where the sample rate is <em class="calibre7">f<sub class="calibre12">s</sub></em> = 8 kHz, as shown in <a href="#calibre_link-561">Figure D-5(a)</a>. After performing a 100-point DFT, and computing the spectral magnitude-squared sample values, we obtain the positive-frequency |<em class="calibre7">X</em>(<em class="calibre7">m</em>)|<sup class="calibre10">2</sup> power spectral samples depicted in <a href="#calibre_link-561">Figure D-5(b)</a>.</p>
<p class="caption"><a id="calibre_link-3502"></a><a id="calibre_link-561"></a><strong class="calibre9">Figure D-5</strong> SNR estimation example: (a) noisy time-domain sinusoid; (b) 100-point DFT power samples.</p>
<p class="image"><img src="images/001750.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">Next we determine a Threshold power value, the dashed line in <a href="#calibre_link-561">Figure D-5(b)</a>, above which only signal-related power samples exist and below which are the noise-only power samples. The estimated SNR of <em class="calibre7">x</em>(<em class="calibre7">n</em>) is then</p>
<p class="caption"><a id="calibre_link-562"></a>(D-19)</p>
<p class="image"><img src="images/001217.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">The SNR measured in dB is found using</p>
<p class="caption"><a id="calibre_link-3503"></a>(D-20)</p>
<p class="image"><img src="images/000256.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">There are several practical topics to keep in mind when estimating SNR by way of frequency-domain samples:</p>
<p class="indenthangingb">• For computational-efficiency reasons, the length of <em class="calibre7">x</em>(<em class="calibre7">n</em>) should be an integer power of two so that fast Fourier transforms (FFTs) can be used to obtain an |<em class="calibre7">X</em>(<em class="calibre7">m</em>)|<sup class="calibre10">2</sup> sequence.</p>
<p class="indenthangingb">• Due to the spectral symmetry of real-only time samples, we need only examine the |<em class="calibre7">X</em>(<em class="calibre7">m</em>)|<sup class="calibre10">2</sup> power samples in the range 0 ≤ <em class="calibre7">m</em> ≤ <em class="calibre7">N</em>/2, i.e., positive frequency.</p>
<p class="indenthangingb">• The Threshold value should be set such that as many of the signal power samples as possible, including any harmonics of the fundamental signal, are above that Threshold value.</p>
<p class="indenthangingb">• If we repeat our SNR estimation computation on multiple non-overlapping <em class="calibre7">N</em>-sample <em class="calibre7">x</em>(<em class="calibre7">n</em>) sequences, we’ll see a noticeable variation (variance) <a id="calibre_link-2377"></a>in the various SNR estimation results. To improve the accuracy, and repeatability, of our SNR estimation it’s prudent to collect many blocks of <em class="calibre7">N</em>-sample <em class="calibre7">x</em>(<em class="calibre7">n</em>) sequences and perform many FFTs to compute multiple |<em class="calibre7">X</em>(<em class="calibre7">m</em>)| magnitude sequences. Then those multiple |<em class="calibre7">X</em>(<em class="calibre7">m</em>)| sequences are averaged before computing a single |<em class="calibre7">X</em>(<em class="calibre7">m</em>)|<sup class="calibre10">2</sup> power sequence for use in <a href="#calibre_link-562">Eq. (D-19)</a>. The idea is to improve the accuracy (reduce the variance) of our SNR estimations by way of averaging as indicated by <a href="#calibre_link-542">Eq. (D-2)</a>.</p>
<p class="calibre3"><a id="calibre_link-3504"></a></p>
<h4 class="calibre13">D.5.3 Controlling Test Signal SNR in Software</h4>
<p class="calibre3">For completeness, below are methods for adjusting the SNR of a real-valued discrete test signal generated in software. Here’s what we mean. Assume we have generated a noise-contaminated zero-mean signal sequence <em class="calibre7">x</em>(<em class="calibre7">n</em>) = <em class="calibre7">x</em><sub class="calibre12">s</sub>(<em class="calibre7">n</em>) + <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>), where <em class="calibre7">x</em><sub class="calibre12">s</sub>(<em class="calibre7">n</em>) are noise-free signal samples and <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>) are noise-only samples. We can adjust the SNR of <em class="calibre7">x</em>(<em class="calibre7">n</em>) to a desired value of <em class="calibre7">SNR</em><sub class="calibre12">new</sub>, measured in dB, by scaling the <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>) noise samples as</p>
<p class="caption"><a id="calibre_link-3505"></a>(D-21)</p>
<p class="image"><img src="images/001095.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">where</p>
<p class="caption"><a id="calibre_link-563"></a>(D-22)</p>
<p class="image"><img src="images/000152.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">So the SNR of the new <em class="calibre7">x</em><sub class="calibre12">new</sub>(<em class="calibre7">n</em>) = <em class="calibre7">x</em><sub class="calibre12">s</sub>(<em class="calibre7">n</em>) + <em class="calibre7">x</em><sub class="calibre12">n,new</sub>(<em class="calibre7">n</em>) sequence will be <em class="calibre7">SNR</em><sub class="calibre12">new</sub> dB where the original <em class="calibre7">x</em><sub class="calibre12">s</sub>(<em class="calibre7">n</em>) noise-free samples remain unchanged. Notice that the ratio in <a href="#calibre_link-563">Eq. (D-22)</a> is the linear (not dB) SNR of the original <em class="calibre7">x</em>(<em class="calibre7">n</em>) sequence.</p>
<p class="calibre3">In a similar manner, we scale the original <em class="calibre7">x</em><sub class="calibre12">s</sub>(<em class="calibre7">n</em>) noise-free samples as</p>
<p class="caption"><a id="calibre_link-3506"></a>(D-23)</p>
<p class="image"><img src="images/000991.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">so that the SNR of the new <em class="calibre7">x</em><sub class="calibre12">new</sub>(<em class="calibre7">n</em>) = <em class="calibre7">x</em><sub class="calibre12">s,new</sub>(<em class="calibre7">n</em>) + <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>) sequence will be the desired <em class="calibre7">SNR</em><sub class="calibre12">new</sub> dB. In this case the original <em class="calibre7">x</em><sub class="calibre12">n</sub>(<em class="calibre7">n</em>) noise samples remain unchanged.</p>
<p class="calibre3"><a id="calibre_link-556"></a></p>
<h3 class="calibre6">D.6 The Mean and Variance of Random Functions</h3>
<p class="calibre3">To determine the mean or variance of a random function, we use what’s called the <em class="calibre7">probability density function</em>. The probability density function (PDF) is a measure of the likelihood of a particular value occurring in some function. <a id="calibre_link-3507"></a>We can explain this concept with simple examples of flipping a coin or throwing dice as illustrated in <a href="#calibre_link-564">Figures D-6(a)</a> and (b). The result of flipping a coin can only be one of two possibilities: heads or tails. <a href="#calibre_link-564">Figure D-6(a)</a> indicates this PDF and shows that the probability (likelihood) is equal to one-half for both heads and tails. That is, we have an equal chance of the coin side facing up being heads or tails. The sum of those two probability values is one, meaning that there’s a 100 percent probability that either a head or a tail will occur.</p>
<p class="caption"><a id="calibre_link-564"></a><strong class="calibre9">Figure D-6</strong> Simple probability density functions: (a) probability of flipping a single coin; (b) probability of a particular sum of the upper faces of two dice; (c) probability of the order of birth of the girl and her sibling.</p>
<p class="image"><img src="images/000775.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3"><a href="#calibre_link-564">Figure D-6(b)</a> shows the probability of a particular sum of the upper faces when we throw a pair of dice. This probability function is not uniform because, for example, we’re six times more likely to have the die faces sum to seven than sum to two (snake eyes).</p>
<p class="calibre3">We can say that after tossing the dice a large number of times, we should expect that 6/36 = 16.7 percent of those tosses would result in sevens, and 1/36 = 2.8 percent of the time we’ll get snake eyes. The sum of those 11 probability values in <a href="#calibre_link-564">Figure D-6(b)</a> is also one, telling us that this PDF accounts for all (100 percent) of the possible outcomes of throwing the dice.</p>
<p class="calibre3">The fact that PDFs must account for all possible result conditions is emphasized in an interesting way in <a href="#calibre_link-564">Figure D-6(c)</a>. Suppose a woman says, “Of my two children, one is a girl. What’s the probability that my daughter has a sister?” Be careful now&mdash;curiously enough, the answer to this controversial question is not a 50-50 chance. There are more possibilities to consider than the girl just having a brother or a sister. We can think of all the possible combinations of birth order of two children such that one child is a girl. Because we don’t know the gender of the first-born child, there are three gender order <a id="calibre_link-3508"></a>possibilities: girl, then boy; boy, then girl; and girl, then girl as shown in <a href="#calibre_link-564">Figure D-6(c)</a>. So the possibility of the daughter having a sister is 1/3 instead of 1/2! (Believe it.) Again, the sum of those three 1/3rd probability values is one.</p>
<p class="calibre3">Two important features of PDFs are illustrated by the examples in <a href="#calibre_link-564">Figure D-6</a>: PDFs are always positive and the area under their <em class="calibre7">curves</em> must be equal to unity. The very concept of PDFs make them a positive <em class="calibre7">likelihood</em> that a particular result will occur, and the fact that some result must occur is equivalent to saying that there’s a probability of one (100 percent chance) that we’ll have that result. For continuous probability density functions, <em class="calibre7">p</em>(<em class="calibre7">x</em>), we indicate these two characteristics by</p>
<p class="caption"><a id="calibre_link-3509"></a>(D-24)</p>
<p class="image"><img src="images/000675.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">and</p>
<p class="caption"><a id="calibre_link-569"></a>(D-25)</p>
<p class="image"><img src="images/000887.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">In <a href="#calibre_link-565">Section D.1</a> we illustrated how to calculate the average (mean) and variance of discrete samples. We can also determine these statistical measures for a random function <em class="calibre7">x</em> if we know the PDF of that function. Using μ<em class="calibre7"><sub class="calibre12">x</sub></em> to denote the average of a random function of <em class="calibre7">x</em>, μ<em class="calibre7"><sub class="calibre12">x</sub></em> is defined as</p>
<p class="caption"><a id="calibre_link-567"></a>(D-26)</p>
<p class="image"><img src="images/001726.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">and the variance of <em class="calibre7">x</em> is defined as<a href="#calibre_link-566">[4]</a></p>
<p class="caption"><a id="calibre_link-568"></a>(D-27)</p>
<p class="image"><img src="images/000769.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">In digital signal processing, we’ll encounter continuous probability density functions that are uniform in value similar to the examples in <a href="#calibre_link-549">Figure D-3</a>. In these cases it’s easy to use <a href="#calibre_link-567">Eqs. (D-26)</a> and <a href="#calibre_link-568">(D-27)</a> to determine their average and variance. <a href="#calibre_link-25">Figure D-7</a> illustrates a uniform continuous PDF indicating <a id="calibre_link-2296"></a>a random function whose values have an equal probability of being anywhere in the range from &ndash;<em class="calibre7">a</em> to <em class="calibre7">b</em>.</p>
<p class="caption"><a id="calibre_link-25"></a><strong class="calibre9">Figure D-7</strong> Continuous uniform probability density function.</p>
<p class="image"><img src="images/001631.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">From <a href="#calibre_link-569">Eq. (D-25)</a> we know that the area under the curve must be unity (i.e., the probability is 100 percent that the value will be somewhere under the curve). So the amplitude of <em class="calibre7">p</em>(<em class="calibre7">x</em>) must be the area divided by the width, or <em class="calibre7">p</em>(<em class="calibre7">x</em>) = 1/(<em class="calibre7">b</em> + <em class="calibre7">a</em>). From <a href="#calibre_link-567">Eq. (D-26)</a> the average of this <em class="calibre7">p</em>(<em class="calibre7">x</em>) is</p>
<p class="caption"><a id="calibre_link-570"></a>(D-28)</p>
<p class="image"><img src="images/001625.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">which happens to be the midpoint in the range from &ndash;<em class="calibre7">a</em> to <em class="calibre7">b</em>. The variance of the PDF in <a href="#calibre_link-25">Figure D-7</a> is</p>
<p class="caption"><a id="calibre_link-26"></a>(D-29)</p>
<p class="image"><img src="images/000648.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">We use the results of <a href="#calibre_link-570">Eqs. (D-28)</a> and <a href="#calibre_link-26">(D-29)</a> in <a href="#calibre_link-290">Chapter 9</a> to analyze the errors induced by quantization from analog-to-digital converters, and the effects of finite word lengths of hardware registers.</p>
<p class="calibre3"><a id="calibre_link-728"></a></p>
<h3 class="calibre6">D.7 The Normal Probability Density Function</h3>
<p class="calibre3">A probability density function (PDF) that’s so often encountered in nature deserves our attention. This function is so common that it’s actually called the <em class="calibre7">normal</em> PDF and is also sometimes called the <em class="calibre7">Gaussian</em> PDF. (A scheme for generating discrete data to fit this function is discussed in <a href="#calibre_link-105">Section 13.12</a>.)</p>
<p class="calibre3"><a id="calibre_link-3510"></a>This function, whose shape is shown in <a href="#calibre_link-571">Figure D-8</a>, is important because random data having this distribution is very useful in testing both software algorithms and hardware processors. The normal PDF is defined mathematically by</p>
<p class="caption"><a id="calibre_link-571"></a><strong class="calibre9">Figure D-8</strong> A normal PDF with mean = μ<em class="calibre7"><sub class="calibre12">x</sub></em> and standard deviation = σ.</p>
<p class="image"><img src="images/000657.jpg" alt="image" class="calibre2" /></p>
<p class="caption"><a id="calibre_link-3511"></a>(D-30)</p>
<p class="image"><img src="images/001504.jpg" alt="image" class="calibre2" /></p>
<p class="calibre3">The area under the curve is one and the percentages at the bottom of <a href="#calibre_link-571">Figure D-8</a> tell us that, for random functions having a normal distribution, there’s a 68.27 percent chance that any particular value of <em class="calibre7">x</em> will differ from the mean by ≤σ. Likewise, 99.73 percent of all the <em class="calibre7">x</em> data values will be within 3σ of the mean μ<em class="calibre7"><sub class="calibre12">x</sub></em>.</p>
<p class="calibre3"><a id="calibre_link-3512"></a></p>
<h3 class="calibre6">References</h3>
<p class="chapterendnote"><a id="calibre_link-541"></a>[1] Papoulis, A. <em class="calibre7">Probability Random Variables, and Stochastic Processes</em>, McGraw-Hill, New York, 1965, pp. 189, pp. 266&ndash;268.</p>
<p class="chapterendnote"><a id="calibre_link-547"></a>[2] Miller, Irwin, and Freund, John. <em class="calibre7">Probability and Statistics for Engineers</em>, 2nd ed., Prentice Hall, Englewood Cliffs, New Jersey, 1977.</p>
<p class="chapterendnote"><a id="calibre_link-551"></a>[3] Meyer, B. <em class="calibre7">Introductory Probability and Statistical Applications</em>, Addison-Wesley, Reading, Massachusetts, 1965, pp. 122&ndash;125.</p>
<p class="chapterendnote"><a id="calibre_link-566"></a>[4] Bendat, Julius, and Piersol, Allen. <em class="calibre7">Measurement and Analysis of Random Data</em>, John Wiley and Sons, New York, 1972.</p>
</div>


</body></html>
